{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa592a4",
   "metadata": {},
   "source": [
    "## 2. Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3541345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /usr/local/anaconda3/lib/python3.13/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/anaconda3/lib/python3.13/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: spacy in /usr/local/anaconda3/lib/python3.13/site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (2.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/anaconda3/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/anaconda3/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/anaconda3/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/anaconda3/lib/python3.13/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/anaconda3/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/anaconda3/lib/python3.13/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: nltk in /usr/local/anaconda3/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/anaconda3/lib/python3.13/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/anaconda3/lib/python3.13/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/anaconda3/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/anaconda3/lib/python3.13/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install spacy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467f18e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/maru/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/maru/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  #  para lematización\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c98a6c",
   "metadata": {},
   "source": [
    "### 2.1 Cargo el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d344af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 reviews\n",
      "Columnas: ['review', 'rating']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sculpting Crean Use this product and find that...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keep your money Foe the price one expects more...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fell apart after a year Was good while it last...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Five Stars Works beautifully. Great for my cli...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Worst Product I recently purchased this produc...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating\n",
       "0  Sculpting Crean Use this product and find that...     5.0\n",
       "1  Keep your money Foe the price one expects more...     1.0\n",
       "2  Fell apart after a year Was good while it last...     1.0\n",
       "3  Five Stars Works beautifully. Great for my cli...     5.0\n",
       "4  Worst Product I recently purchased this produc...     1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el dataset balanceado desde el Notebook 1\n",
    "df = pd.read_pickle('Outputs/data/df_beauty_balanced.pkl')\n",
    "print(f\"{len(df)} reviews\")\n",
    "print(f\"Columnas: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e6d71",
   "metadata": {},
   "source": [
    "### 2.2 Etiqueto las rating en positivo [1] y negativo [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b3274b",
   "metadata": {},
   "source": [
    "- `0`: las menores de 3\n",
    "- `1`: las mayor o igual a 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f843e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentiment(rating):\n",
    "    #Convierte a etiqueta binaria\n",
    "    if rating < 3:\n",
    "        return 1  \n",
    "    else:\n",
    "        return 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a561d",
   "metadata": {},
   "source": [
    "### 2.3 Convierto a minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lowercase_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_minusculas(texto):\n",
    "    return texto.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lowercase_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sculpting Crean Use this product and find that when I run out, I notice the difference in the tautness of my skin, especially around mouth and neck.\n",
      "Procesado: sculpting crean use this product and find that when i run out, i notice the difference in the tautness of my skin, especially around mouth and neck.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "print(f\"Original: {df.iloc[0]['review']}\")\n",
    "print(f\"Procesado: {a_minusculas(df.iloc[0]['review'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf3a1a",
   "metadata": {},
   "source": [
    "### 2.4 Aplico beatifulSoup para eliminar las etiquetas HTLM que pueda haber en el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06eb5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text(separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dff2baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sculpting Crean Use this product and find that when I run out, I notice the difference in the tautness of my skin, especially around mouth and neck.\n",
      "Procesado: Sculpting Crean Use this product and find that when I run out, I notice the difference in the tautness of my skin, especially around mouth and neck.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "print(f\"Original: {df.iloc[0]['review']}\")\n",
    "print(f\"Procesado: {remove_html_tags(df.iloc[0]['review'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "punctuation_section",
   "metadata": {},
   "source": [
    "### 2.5 Elimino signos de puntuación del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "punctuation_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_puntuacion(texto):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return texto.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "punctuation_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Nothing Special Didn’t work no where near like it seemed to for others. Did nothing but weigh my hair down with greasy product\n",
      "Procesado: Nothing Special Didn’t work no where near like it seemed to for others Did nothing but weigh my hair down with greasy product\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "print(f\"Original: {df.iloc[62]['review']}\")\n",
    "print(f\"Procesado: {eliminar_puntuacion(df.iloc[62]['review'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unicode_section",
   "metadata": {},
   "source": [
    "### 2.6 Estandarizo caracteres especiales y elimino tildes (á→a, é→e, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unicode_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_unicode(texto):\n",
    "    return texto.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unicode_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sculpting Crean Use this product and find that when I run out, I notice the difference in the tautness of my skin, especially around mouth and neck.\n",
      "Procesado: Sculpting Crean Use this product and find that when I run out, I notice the difference in the tautness of my skin, especially around mouth and neck.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "print(f\"Original: {df.iloc[0]['review']}\")\n",
    "print(f\"Procesado: {normalizar_unicode(df.iloc[0]['review'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numbers_section",
   "metadata": {},
   "source": [
    "### 2.7 Eliminar números del texto \n",
    "\n",
    "- Los números en reviews de productos de belleza suelen ser poco informativos para análisis de sentimiento y no aportan valor semantico al modelo. \n",
    "- Reducimos ruido, simplificamos vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "numbers_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_numeros(texto):\n",
    "    return re.sub(r'\\d+', '', texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "numbers_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Worked great ONE time Item worked great the first time, but the next day when I tried to turn it on nothing worked. returned\n",
      "Procesado: Worked great ONE time Item worked great the first time, but the next day when I tried to turn it on nothing worked. returned\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "print(f\"Original: {df.iloc[2025]['review']}\")\n",
    "print(f\"Procesado: {eliminar_numeros(df.iloc[2025]['review'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spaces_section",
   "metadata": {},
   "source": [
    "### 2.8 Eliminar espacios blancos y espacios al inicio/final del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a118c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_espacios(texto):\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    return texto.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a49d4d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Works GREAT It's very stretchy\n",
      "Procesado: Works GREAT It's very stretchy\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "print(f\"Original: {df.iloc[5520]['review']}\")\n",
    "print(f\"Procesado: {normalizar_espacios(df.iloc[5520]['review'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopwords_section",
   "metadata": {},
   "source": [
    "### 2.9 Eliminar stopwords\n",
    "\n",
    "Pero **manteniendo palabras de sentimiento negativo** como 'not', 'no', 'never', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stopwords_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar stopwords si no están disponibles\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def eliminar_stopwords(texto):\n",
    "    # Obtener stopwords en inglés\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Palabras de negación a mantener\n",
    "    negation_words = {'not', \"n't\", 'no', 'never', 'neither', 'nobody', 'nothing', \n",
    "                      'nowhere', 'none', 'nor', \"don't\", \"doesn't\", \"didn't\", \n",
    "                      \"won't\", \"wouldn't\", \"shouldn't\", \"can't\", \"cannot\", \"couldn't\"}\n",
    "                      \n",
    "    stop_words = stop_words - negation_words\n",
    "    \n",
    "    # Filtrar palabras\n",
    "    palabras = texto.split()\n",
    "    palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(palabras_filtradas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stopwords_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sculpting Crean Use this product and find that when I run out, I notice the difference in the tautness of my skin, especially around mouth and neck.\n",
      "Procesado: Sculpting Crean Use product find run out, notice difference tautness skin, especially around mouth neck.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "print(f\"Original: {df.iloc[0]['review']}\")\n",
    "print(f\"Procesado: {eliminar_stopwords(df.iloc[0]['review'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenization_section",
   "metadata": {},
   "source": [
    "### 2.10 Tokenización separando el texto en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "tokenization_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenizar(texto):\n",
    "    return word_tokenize(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tokenization_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sculpting Crean Use this product and find that when I run out, I notice the difference in the tautness of my skin, especially around mouth and neck.\n",
      "Procesado: ['Sculpting', 'Crean', 'Use', 'this', 'product', 'and', 'find', 'that', 'when', 'I', 'run', 'out', ',', 'I', 'notice', 'the', 'difference', 'in', 'the', 'tautness', 'of', 'my', 'skin', ',', 'especially', 'around', 'mouth', 'and', 'neck', '.']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "print(f\"Original: {df.iloc[0]['review']}\")\n",
    "print(f\"Procesado: {tokenizar(df.iloc[0]['review'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lemmatization_section",
   "metadata": {},
   "source": [
    "### 2.11 Lematización para usar en los modelos de ML con TF-IDF (Reduzco la dimensionalidad y ayudo al modelo a aprender mejor los datos aumentando la coincidencia de palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lemmatization_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lematizar_texto(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "lemmatization_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sculpting Crean Use this product and find that when I run out, I notice the difference in the tautness of my skin, especially around mouth and neck.\n",
      "Procesado: ['Sculpting', 'Crean', 'Use', 'this', 'product', 'and', 'find', 'that', 'when', 'I', 'run', 'out', ',', 'I', 'notice', 'the', 'difference', 'in', 'the', 'tautness', 'of', 'my', 'skin', ',', 'especially', 'around', 'mouth', 'and', 'neck', '.']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "print(f\"Original: {df.iloc[0]['review']}\")\n",
    "tokens = tokenizar(df.iloc[0]['review'])\n",
    "print(f\"Procesado: {lematizar_texto(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter_section",
   "metadata": {},
   "source": [
    "### 2.12 Eliminar tokens de menos de 3 caracteres (tokens cortos)\n",
    "\n",
    "- Tokens de 1-2 caracteres suelen ser poco informativos (artículos, preposiciones)\n",
    "- Mantiene palabras significativas como 'not', 'bad', 'buy', 'use'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "filter_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_tokens_cortos(tokens, min_length=3):\n",
    "    return [token for token in tokens if len(token) >= min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "filter_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Sculpting Crean Use this product and find that when I run out, I notice the difference in the tautness of my skin, especially around mouth and neck.\n",
      "Procesado: ['Sculpting', 'Crean', 'Use', 'this', 'product', 'and', 'find', 'that', 'when', 'run', 'out', 'notice', 'the', 'difference', 'the', 'tautness', 'skin', 'especially', 'around', 'mouth', 'and', 'neck']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "print(f\"Original: {df.iloc[0]['review']}\")\n",
    "tokens = tokenizar(df.iloc[0]['review'])\n",
    "print(f\"Procesado: {filtrar_tokens_cortos(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline_section",
   "metadata": {},
   "source": [
    "## 2.2 Pipeline completo de preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pipeline_func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesado(texto, \n",
    "                   usar_stopwords=True, \n",
    "                   usar_lematizacion=True, \n",
    "                   filtrar_cortos=True,\n",
    "                   min_length=3):\n",
    "    \n",
    "    # Verificar que la review no esté vacía\n",
    "    if not texto or not isinstance(texto, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Pasar a minusculas\n",
    "    texto = a_minusculas(texto)\n",
    "    \n",
    "    # Eliminar etiquetas\n",
    "    texto = remove_html_tags(texto)\n",
    "    \n",
    "    # Eliminar caracteres especiales\n",
    "    texto = normalizar_unicode(texto)\n",
    "    \n",
    "    # Eliminar puntuación\n",
    "    texto = eliminar_puntuacion(texto)\n",
    "    \n",
    "    # Eliminar números\n",
    "    texto = eliminar_numeros(texto)\n",
    "    \n",
    "    # Normalizar espacios\n",
    "    texto = normalizar_espacios(texto)\n",
    "    \n",
    "    # Eliminar stopwords (opcional)\n",
    "    if usar_stopwords:\n",
    "        texto = eliminar_stopwords(texto)\n",
    "    \n",
    "    # Tokenización\n",
    "    tokens = tokenizar(texto)\n",
    "    \n",
    "    # Lematización (opcional)\n",
    "    if usar_lematizacion:\n",
    "        tokens = lematizar_texto(tokens)\n",
    "    \n",
    "    # Filtrar tokens cortos (opcional)\n",
    "    if filtrar_cortos:\n",
    "        tokens = filtrar_tokens_cortos(tokens, min_length)\n",
    "    \n",
    "    # Unir tokens en texto\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "pipeline_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTO ORIGINAL:\n",
      "Worked great ONE time Item worked great the first time, but the next day when I tried to turn it on nothing worked. returned\n",
      "TEXTO PREPROCESADO:\n",
      "worked great one time item worked great first time next day tried turn nothing worked returned\n",
      "SIN STOPWORDS NI LEMATIZACIÓN:\n",
      "worked great one time item worked great the first time but the next day when tried turn nothing worked returned\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo completo del pipeline\n",
    "review_ejemplo=df.iloc[2025]['review']\n",
    "\n",
    "print(\"TEXTO ORIGINAL:\")\n",
    "print(review_ejemplo)\n",
    "\n",
    "print(\"TEXTO PREPROCESADO:\")\n",
    "print(preprocesado(review_ejemplo))\n",
    "\n",
    "print(\"SIN STOPWORDS NI LEMATIZACIÓN:\")\n",
    "print(preprocesado(review_ejemplo, usar_stopwords=False, usar_lematizacion=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408260ff",
   "metadata": {},
   "source": [
    "## 2.3 Aplicar preprocesado al dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "apply_preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la función de preprocesado\n",
    "df['review_processed_ML'] = df['review'].apply(\n",
    "    lambda x: preprocesado(x, usar_stopwords=True, usar_lematizacion=True, filtrar_cortos=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "321a225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: Sculpting Crean Use this product and find that when I run out, I notice the difference in the tautness of my skin, especially around mouth and neck.\n",
      "PROCESADO: sculpting crean use product find run notice difference tautness skin especially around mouth neck\n"
     ]
    }
   ],
   "source": [
    "#Ejemplos:\n",
    "print(f\"ORIGINAL: {df.iloc[0]['review'][:150]}\")\n",
    "print(f\"PROCESADO: {df.iloc[0]['review_processed_ML'][:150]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac3528",
   "metadata": {},
   "source": [
    "#### Revisamos si hay alguna review vacia y eliminamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "check_empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reviews vacías después del preprocesado: 6\n"
     ]
    }
   ],
   "source": [
    "empty_reviews = df[df['review_processed_ML'].str.strip() == '']\n",
    "print(f\"\\nReviews vacías después del preprocesado: {len(empty_reviews)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c8377fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de reviews después del filtrado: 5994\n"
     ]
    }
   ],
   "source": [
    "df = df[df['review_processed_ML'].notna() & (df['review_processed_ML'].str.strip() != '')]\n",
    "print(f\"Total de reviews después del filtrado: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "vocab_stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario original: 19,121 palabras únicas\n",
      "Vocabulario procesado: 9,569 palabras únicas\n",
      "Reducción: 50.0% palabras únicas\n"
     ]
    }
   ],
   "source": [
    "# Calcular reducción de vocabulario\n",
    "from collections import Counter\n",
    "\n",
    "# Vocabulario original\n",
    "vocab_original = set()\n",
    "for text in df['review']:\n",
    "    vocab_original.update(str(text).lower().split())\n",
    "\n",
    "# Vocabulario procesado\n",
    "vocab_procesado = set()\n",
    "for text in df['review_processed_ML']:\n",
    "    vocab_procesado.update(str(text).split())\n",
    "\n",
    "print(f\"Vocabulario original: {len(vocab_original):,} palabras únicas\")\n",
    "print(f\"Vocabulario procesado: {len(vocab_procesado):,} palabras únicas\")\n",
    "print(f\"Reducción: {(1 - len(vocab_procesado)/len(vocab_original))*100:.1f}% palabras únicas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84326e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar etiquetado al dataframe usando la función label_sentiment\n",
    "df['label_sentiment'] = df['rating'].apply(label_sentiment)\n",
    "\n",
    "# Quedarse solo con las columnas especificadas\n",
    "df_ML = df[['review_processed_ML','label_sentiment']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23b2ad37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_processed_ML</th>\n",
       "      <th>label_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sculpting crean use product find run notice di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>keep money foe price one expects eye shadowyou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fell apart year good lasted wasnt long brush f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>five star work beautifully great client sensit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst product recently purchased product terri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 review_processed_ML  label_sentiment\n",
       "0  sculpting crean use product find run notice di...                0\n",
       "1  keep money foe price one expects eye shadowyou...                1\n",
       "2  fell apart year good lasted wasnt long brush f...                1\n",
       "3  five star work beautifully great client sensit...                0\n",
       "4  worst product recently purchased product terri...                1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ML.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_section",
   "metadata": {},
   "source": [
    "## 2.4 Guardar dataset preprocesado\n",
    "\n",
    "Guardamos el dataset con las reviews preprocesadas para usar en el Notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "save_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preprocesado guardado en: Outputs/data/df_beauty_preprocessed_ML.pkl\n"
     ]
    }
   ],
   "source": [
    "# Guardar dataset preprocesado\n",
    "output_path = 'Outputs/data/df_beauty_preprocessed_ML.pkl'\n",
    "df_ML.to_pickle(output_path)\n",
    "\n",
    "print(f\"Dataset preprocesado guardado en: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP Deep Learning (Python 3.13)",
   "language": "python",
   "name": "nlp-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
