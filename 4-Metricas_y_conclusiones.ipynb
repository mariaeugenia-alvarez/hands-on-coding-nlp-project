{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "## 4. Metricas y Conclusiones\n",
    "\n",
    "En este notebook realizaremos una comparativa exhaustiva de los modelos entrenados en los notebooks anteriores:\n",
    "- **Machine Learning**: Logistic Regression y Naive Bayes\n",
    "- **Deep Learning**: GRU y LSTM con Word2Vec\n",
    "\n",
    "El objetivo es analizar el rendimiento de cada modelo en el contexto del analisis de sentimiento de reviews de productos de belleza, utilizando un dataset balanceado de aproximadamente 6000 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "### 4.1 Setup y Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Configuracion de visualizacion\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_results",
   "metadata": {},
   "source": [
    "### 4.2 Carga de Resultados\n",
    "\n",
    "Cargamos los resultados guardados de los modelos de Machine Learning y Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "load_ml_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados Deep Learning:\n",
      "             Model  Accuracy  Precision    Recall  F1-Score   ROC-AUC\n",
      "0              GRU  0.512222   0.506329  0.977778  0.667172  0.511862\n",
      "1  LSTM + Word2Vec  0.627778   0.574194  0.988889  0.726531  0.634600\n",
      "\n",
      "Shape: (2, 6)\n"
     ]
    }
   ],
   "source": [
    "# Resultados de Deep Learning (guardados en CSV)\n",
    "df_dl = pd.read_csv('Outputs/results_deep_learning.csv')\n",
    "print(\"Resultados Deep Learning:\")\n",
    "print(df_dl)\n",
    "print(f\"\\nShape: {df_dl.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "create_ml_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados Machine Learning:\n",
      "                 Model  Accuracy  Precision  Recall  F1-Score  ROC-AUC\n",
      "0  Logistic Regression    0.8533      0.855  0.8533     0.853    0.925\n",
      "1          Naive Bayes    0.8200      0.818  0.8200     0.819    0.895\n"
     ]
    }
   ],
   "source": [
    "ml_results = {\n",
    "    'Model': ['Logistic Regression', 'Naive Bayes'],\n",
    "    'Accuracy': [0.8533, 0.8200],  # Valores aproximados del notebook 3a\n",
    "    'Precision': [0.8550, 0.8180],\n",
    "    'Recall': [0.8533, 0.8200],\n",
    "    'F1-Score': [0.8530, 0.8190],\n",
    "    'ROC-AUC': [0.9250, 0.8950]  # Valores aproximados\n",
    "}\n",
    "\n",
    "df_ml = pd.DataFrame(ml_results)\n",
    "print(\"Resultados Machine Learning:\")\n",
    "print(df_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "### 4.3 Comparativa General de Modelos\n",
    "\n",
    "Unimos los resultados de todos los modelos para realizar una comparativa completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "merge_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARATIVA COMPLETA DE MODELOS\n",
      "              Model  Accuracy  Precision   Recall  F1-Score  ROC-AUC Tipo\n",
      "Logistic Regression  0.853300   0.855000 0.853300  0.853000 0.925000   ML\n",
      "        Naive Bayes  0.820000   0.818000 0.820000  0.819000 0.895000   ML\n",
      "                GRU  0.512222   0.506329 0.977778  0.667172 0.511862   DL\n",
      "    LSTM + Word2Vec  0.627778   0.574194 0.988889  0.726531 0.634600   DL\n"
     ]
    }
   ],
   "source": [
    "# Combinar resultados\n",
    "df_all = pd.concat([df_ml, df_dl], ignore_index=True)\n",
    "\n",
    "# AÃ±adir columna de tipo de modelo\n",
    "df_all['Tipo'] = ['ML', 'ML', 'DL', 'DL']\n",
    "\n",
    "print(\"COMPARATIVA COMPLETA DE MODELOS\")\n",
    "print(df_all.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "### 4.7 Conclusiones Finales\n",
    "\n",
    "#### Contexto del Estudio\n",
    "Este analisis se ha realizado sobre un dataset balanceado de aproximadamente 6000 reviews de productos de belleza, con el objetivo de clasificar el sentimiento (positivo/negativo) de las opiniones de los usuarios.\n",
    "\n",
    "#### Rendimiento de los Modelos\n",
    "\n",
    "**Machine Learning (Modelos Tradicionales):**\n",
    "- **Logistic Regression**: Ha demostrado ser el modelo mas robusto y consistente, con un accuracy superior al 85%. Su rendimiento equilibrado en todas las metricas lo convierte en una opcion solida para este tipo de tarea.\n",
    "- **Naive Bayes**: Aunque ligeramente inferior a Logistic Regression, mantiene un rendimiento aceptable (aproximadamente 82 porciento de accuracy) y destaca por su rapidez de entrenamiento.\n",
    "\n",
    "**Deep Learning (Redes Neuronales):**\n",
    "- **LSTM con Word2Vec**: Este modelo ha mostrado un rendimiento moderado (aproximadamente 63% de accuracy), beneficiandose de los embeddings pre-entrenados para capturar relaciones semanticas entre palabras.\n",
    "- **GRU**: Ha presentado el rendimiento mas bajo de todos los modelos evaluados (50% de accuracy), lo que sugiere que requiere mas datos o ajustes en la arquitectura para este problema especifico.\n",
    "\n",
    "\n",
    "#### Recomendaciones\n",
    "\n",
    "**Para este dataset especifico (6000 reviews):**\n",
    "- **Modelo recomendado**: Logistic Regression con TF-IDF\n",
    "- **Justificacion**: Mejor balance entre rendimiento, velocidad de entrenamiento, interpretabilidad y recursos computacionales necesarios.\n",
    "\n",
    "- Si se dispone de un dataset significativamente mayor (>50,000 reviews), seria recomendable reevaluar los modelos de Deep Learning.\n",
    "- Considerar arquitecturas mas complejas como BERT o transformers si se cuenta con recursos computacionales adecuados y grandes volumenes de datos.\n",
    "\n",
    "**Mejoras potenciales:**\n",
    "1. **Aumento de datos**: Recopilar mas reviews para mejorar el rendimiento de los modelos de Deep Learning.\n",
    "2. **Ensemble methods**: Combinar predicciones de multiples modelos podria mejorar el rendimiento global.\n",
    "3. **Ajuste de hiperparametros**: Realizar una busqueda mas exhaustiva de hiperparametros optimos para cada modelo.\n",
    "\n",
    "#### Conclusion General\n",
    "\n",
    "Para el analisis de sentimiento de reviews de productos de belleza con un dataset de 6000 muestras balanceadas, los modelos de Machine Learning tradicional, especificamente Logistic Regression, han demostrado ser la opcion mas efectiva por accuracy. Los modelos de Deep Learning, requieren datasets mas grandes para demostrar su verdadero potencial en tareas de procesamiento de lenguaje natural."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP DL (Python 3.11)",
   "language": "python",
   "name": "nlp-dl-stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
